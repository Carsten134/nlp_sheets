{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from utils import show_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the text:\n",
    "got_books = [] \n",
    "\n",
    "for i in range(1, 6):\n",
    "    try:\n",
    "        with open(f\"./data/00{i}ssb.txt\") as f:\n",
    "            got_books.append(f.read())\n",
    "    except:\n",
    "        with open(f\"./data/00{i}ssb.txt\", encoding=\"cp1252\") as f:\n",
    "            got_books.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "got_books = [re.sub(r\"Page \\d+\", \"\", book) for book in got_books] # removing pages\n",
    "got_books = [re.sub(r\"[A-Z ]{2,}\\n\", \"\", book) for book in got_books] # removing chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "lm = WordNetLemmatizer()\n",
    "def preprocessing(text) -> list:\n",
    "    # remove special characters and unwanted control characters and make lower case\n",
    "    processed_t = re.sub(r\"[^A-Za-z ]\", \"\", text)\n",
    "    processed_t = processed_t.lower().split()\n",
    "    # remove stopwords\n",
    "    processed_t = [word for word in processed_t if word not in nltk.corpus.stopwords.words(\"english\")]\n",
    "    # lemmatize\n",
    "    processed_t = [lm.lemmatize(word) for word in processed_t]\n",
    "    \n",
    "    return processed_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_got_books = [preprocessing(book) for book in got_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn processed text into corpus for LDA modelb\n",
    "word_dict = Dictionary(processed_got_books)\n",
    "corpus = [word_dict.doc2bow(book) for book in processed_got_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.012*\"said\" + 0.009*\"would\" + 0.008*\"lord\" + 0.006*\"ser\" + 0.005*\"one\" + 0.005*\"could\" + 0.005*\"hand\" + 0.005*\"ned\" + 0.005*\"king\" + 0.004*\"jon\"')\n",
      "(0, '0.010*\"said\" + 0.008*\"lord\" + 0.007*\"would\" + 0.006*\"ser\" + 0.005*\"hand\" + 0.004*\"could\" + 0.004*\"king\" + 0.004*\"ned\" + 0.004*\"like\" + 0.004*\"one\"')\n",
      "(0, '0.011*\"said\" + 0.008*\"lord\" + 0.007*\"would\" + 0.006*\"ser\" + 0.005*\"one\" + 0.005*\"king\" + 0.005*\"hand\" + 0.005*\"could\" + 0.005*\"eye\" + 0.005*\"jon\"')\n",
      "(0, '0.013*\"said\" + 0.009*\"lord\" + 0.007*\"ser\" + 0.006*\"would\" + 0.005*\"one\" + 0.005*\"could\" + 0.005*\"king\" + 0.004*\"jon\" + 0.004*\"hand\" + 0.004*\"man\"')\n",
      "(0, '0.012*\"said\" + 0.008*\"lord\" + 0.007*\"would\" + 0.005*\"ned\" + 0.005*\"ser\" + 0.004*\"king\" + 0.004*\"jon\" + 0.004*\"could\" + 0.004*\"one\" + 0.004*\"man\"')\n"
     ]
    }
   ],
   "source": [
    "# Task 4\n",
    "ldas_f_b = []\n",
    "for _ in range(5):\n",
    "    # 50 iterations is already the default\n",
    "    # also only train on the first book\n",
    "    lda = LdaModel([corpus[0]], 10, id2word=word_dict, iterations=50)\n",
    "    print(lda.show_topics()[0])\n",
    "    ldas_f_b.append(lda) # reuse this in task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- They are not equal but similar, this is because of the mcmc nature of estimation. We are incoporating randomness into the estimation.\n",
    "- If would use the same seed, the results would be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic model on book 0:\n",
      "(0, '0.012*\"said\" + 0.009*\"lord\" + 0.007*\"would\" + 0.006*\"one\" + 0.006*\"ned\" + 0.004*\"king\" + 0.004*\"jon\" + 0.004*\"ser\" + 0.004*\"eye\" + 0.004*\"like\"')\n",
      "Topic model on book 1:\n",
      "(0, '0.008*\"lord\" + 0.007*\"one\" + 0.007*\"would\" + 0.006*\"man\" + 0.006*\"said\" + 0.006*\"ser\" + 0.005*\"could\" + 0.004*\"back\" + 0.003*\"king\" + 0.003*\"tyrion\"')\n",
      "Topic model on book 2:\n",
      "(0, '0.008*\"said\" + 0.007*\"lord\" + 0.006*\"one\" + 0.005*\"would\" + 0.005*\"well\" + 0.005*\"ser\" + 0.005*\"back\" + 0.004*\"could\" + 0.004*\"man\" + 0.004*\"like\"')\n",
      "Topic model on book 3:\n",
      "(0, '0.008*\"would\" + 0.008*\"one\" + 0.008*\"said\" + 0.007*\"lord\" + 0.005*\"ser\" + 0.004*\"could\" + 0.004*\"man\" + 0.004*\"jaime\" + 0.004*\"king\" + 0.003*\"men\"')\n",
      "Topic model on book 4:\n",
      "(0, '0.008*\"would\" + 0.008*\"one\" + 0.007*\"lord\" + 0.006*\"said\" + 0.006*\"could\" + 0.006*\"men\" + 0.004*\"back\" + 0.004*\"man\" + 0.004*\"jon\" + 0.004*\"eye\"')\n",
      "------------\n",
      "Topic models from task 4:\n",
      "(0, '0.012*\"said\" + 0.009*\"would\" + 0.008*\"lord\" + 0.006*\"ser\" + 0.005*\"one\" + 0.005*\"could\" + 0.005*\"hand\" + 0.005*\"ned\" + 0.005*\"king\" + 0.004*\"jon\"')\n",
      "(0, '0.010*\"said\" + 0.008*\"lord\" + 0.007*\"would\" + 0.006*\"ser\" + 0.005*\"hand\" + 0.004*\"could\" + 0.004*\"king\" + 0.004*\"ned\" + 0.004*\"like\" + 0.004*\"one\"')\n",
      "(0, '0.011*\"said\" + 0.008*\"lord\" + 0.007*\"would\" + 0.006*\"ser\" + 0.005*\"one\" + 0.005*\"king\" + 0.005*\"hand\" + 0.005*\"could\" + 0.005*\"eye\" + 0.005*\"jon\"')\n",
      "(0, '0.013*\"said\" + 0.009*\"lord\" + 0.007*\"ser\" + 0.006*\"would\" + 0.005*\"one\" + 0.005*\"could\" + 0.005*\"king\" + 0.004*\"jon\" + 0.004*\"hand\" + 0.004*\"man\"')\n",
      "(0, '0.012*\"said\" + 0.008*\"lord\" + 0.007*\"would\" + 0.005*\"ned\" + 0.005*\"ser\" + 0.004*\"king\" + 0.004*\"jon\" + 0.004*\"could\" + 0.004*\"one\" + 0.004*\"man\"')\n"
     ]
    }
   ],
   "source": [
    "# Task 5\n",
    "for i in range(len(corpus)):\n",
    "    print(f\"Topic model on book {i}:\")\n",
    "    print(LdaModel([corpus[i]], 10, id2word=word_dict).show_topics()[0])\n",
    "print(\"------------\\nTopic models from task 4:\")\n",
    "for lda in ldas_f_b:\n",
    "    print(lda.show_topics()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
